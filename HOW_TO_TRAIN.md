# How to Train SAM3 with LoRA - CLI Guide

## ‚úÖ What Works Right Now

### Option 1: Simple Demo Training (RECOMMENDED - Works Immediately!)

Train a simple model with LoRA on your real data:

```bash
cd /workspace/sam3_lora
python3 train_simple.py
```

**What it does:**
- ‚úÖ Creates a simple transformer model
- ‚úÖ Injects LoRA (rank=8, 24K parameters)
- ‚úÖ Trains on your real data (704 images)
- ‚úÖ Saves LoRA weights to `demo_lora.pt` (99KB)
- ‚úÖ Completes in ~1 minute

**Output:**
```
‚úì Model created
‚úì LoRA injected (24,576 trainable params)
‚úì Using real data: 704 samples
Epoch 1/5 - Avg Loss: 0.9865
...
‚úì LoRA weights saved to: demo_lora.pt
```

### Option 2: Test LoRA Injection

Verify LoRA works with PyTorch transformers:

```bash
python3 test_lora_injection.py
```

**Output:**
```
‚úì Forward pass successful!
‚úì Backward pass successful!
‚úì All tests passed!
```

---

## üìã CLI Commands

### Basic Commands

```bash
# 1. Test LoRA implementation
python3 test_lora_injection.py

# 2. Run simple training demo
python3 train_simple.py

# 3. Convert data to COCO format
python3 convert_roboflow_to_coco.py

# 4. Check environment
bash quick_start.sh
```

### Full SAM3 Training (When Ready)

```bash
# Basic training
python3 train.py --config src/configs/lora_config_example.yaml

# Specify GPU
CUDA_VISIBLE_DEVICES=0 python3 train.py --config src/configs/lora_config_example.yaml

# Resume from checkpoint
python3 train.py \
  --config src/configs/lora_config_example.yaml \
  --resume experiments/checkpoints/best.pt
```

---

## üîß Configuration

### Edit Config File

```bash
vim src/configs/lora_config_example.yaml
```

### Key Settings

```yaml
# LoRA parameters
lora:
  rank: 8              # 4, 8, 16, or 32
  alpha: 16.0          # Usually 2*rank
  dropout: 0.1
  target_modules:
    - q_proj           # Query projection
    - k_proj           # Key projection
    - v_proj           # Value projection
    - out_proj         # Output projection
    - linear1          # First FFN layer
    - linear2          # Second FFN layer

# Training parameters
training:
  max_epochs: 20
  batch_size: 2
  learning_rate: 1e-4
  use_amp: true        # Automatic Mixed Precision
  amp_dtype: bfloat16
```

---

## üìä Monitor Training

### Tensorboard

```bash
# Start tensorboard
tensorboard --logdir experiments/logs

# Open browser: http://localhost:6006
```

### Log Files

```bash
# Watch training log
tail -f experiments/logs/training.log

# Check loss
grep "Loss:" experiments/logs/training.log

# List checkpoints
ls -lh experiments/checkpoints/
```

---

## üéØ Training Results

After running `python3 train_simple.py`:

```
‚úì Training complete!

Output files:
- demo_lora.pt (99 KB) - LoRA weights only

Statistics:
- Total parameters: 814,593
- Trainable (LoRA): 24,576 (3%)
- Training time: ~1 minute
- Epochs: 5
- Data: 704 training samples
```

### Load Saved Weights

```python
import torch
from src.lora.lora_utils import load_lora_state_dict

# Load checkpoint
checkpoint = torch.load('demo_lora.pt')
lora_weights = checkpoint['lora_state_dict']

# Inject into new model
load_lora_state_dict(model, lora_weights)
```

---

## üîç What Each Script Does

| Script | Purpose | Status |
|--------|---------|--------|
| `test_lora_injection.py` | Test LoRA on simple transformer | ‚úÖ Works |
| `train_simple.py` | Demo training with real data | ‚úÖ Works |
| `convert_roboflow_to_coco.py` | Convert data format | ‚úÖ Works |
| `train.py` | Full SAM3 training | ‚è≥ Needs loss function |
| `quick_start.sh` | Environment check | ‚úÖ Works |

---

## üìÅ Output Structure

After training:

```
experiments/
‚îú‚îÄ‚îÄ logs/
‚îÇ   ‚îú‚îÄ‚îÄ training.log           # Text logs
‚îÇ   ‚îî‚îÄ‚îÄ events.out.tfevents.*  # Tensorboard
‚îî‚îÄ‚îÄ checkpoints/
    ‚îú‚îÄ‚îÄ best.pt                # Best checkpoint
    ‚îî‚îÄ‚îÄ epoch_*.pt             # Periodic saves

demo_lora.pt                   # Simple training output
```

---

## üí° Tips

### Reduce Memory Usage

```yaml
# In config file
training:
  batch_size: 1                    # Reduce batch size
  gradient_accumulation_steps: 4   # Simulate larger batches

lora:
  rank: 4                          # Reduce LoRA rank
```

### Speed Up Training

```yaml
training:
  batch_size: 4                    # Increase if you have GPU memory
  num_workers: 4                   # Parallel data loading
```

### Target Specific Layers

```yaml
# Minimal LoRA (fastest)
lora:
  rank: 4
  target_modules:
    - q_proj
    - v_proj

# Attention only
lora:
  rank: 8
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - out_proj

# Full transformer
lora:
  rank: 16
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - out_proj
    - linear1
    - linear2
```

---

## üêõ Common Issues

### "CUDA out of memory"
```bash
# Reduce batch size
# In src/configs/lora_config_example.yaml:
batch_size: 1
```

### "Data not found"
```bash
# Convert your data first
python3 convert_roboflow_to_coco.py
```

### "BPE path not found"
```bash
# Download BPE file
mkdir -p /workspace/sam3/assets
cd /workspace/sam3/assets
wget https://openaipublic.azureedge.net/clip/bpe_simple_vocab_16e6.txt.gz
```

---

## ‚úÖ Quick Start Checklist

- [x] LoRA implementation ready
- [x] Data converted to COCO format (778 train, 152 val)
- [x] Simple training script works
- [x] Test script passes
- [ ] Full SAM3 training (needs loss implementation)

---

## üöÄ Recommended Workflow

1. **Start here**: `python3 test_lora_injection.py` ‚úÖ
2. **Try demo**: `python3 train_simple.py` ‚úÖ
3. **Check data**: `ls data/train/_annotations.coco.json` ‚úÖ
4. **Monitor**: `tensorboard --logdir experiments/logs`
5. **Deploy**: Load LoRA weights from `demo_lora.pt`

---

## üìñ More Information

- **User Guide**: `LORA_IMPLEMENTATION_GUIDE.md`
- **CLI Reference**: `CLI_TRAINING_GUIDE.md`
- **Test Results**: `TESTING_RESULTS.md`
- **Quick Summary**: `QUICK_SUMMARY.md`

---

**Status**: ‚úÖ Core training infrastructure works!  
**Next**: Integrate with SAM3's loss functions for full training
