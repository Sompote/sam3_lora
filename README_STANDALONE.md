# SAM3 LoRA - Standalone Version

## ğŸ¯ Overview

This is a **standalone implementation** of LoRA (Low-Rank Adaptation) for SAM3 that **does NOT require SAM3 installation**. It's a complete, self-contained package.

## âœ¨ Key Features

âœ… **Standalone** - No external SAM3 dependencies
âœ… **Easy Installation** - Simple pip install
âœ… **Production Ready** - Fully tested and documented
âœ… **Flexible** - Works with any PyTorch model
âœ… **Lightweight** - Minimal dependencies

## ğŸ“¦ Installation

### Option 1: Install from source

```bash
cd /workspace/sam3_lora
pip install -e .
```

### Option 2: Install dependencies only

```bash
pip install -r requirements.txt
```

## ğŸš€ Quick Start

### 1. Test the Installation

```bash
python3 test_standalone.py
```

Expected output:
```
âœ“ Forward pass successful!
âœ“ Backward pass successful!
âœ“ All tests passed!
The standalone package works correctly without SAM3!
```

### 2. Run Standalone Training

```bash
python3 train_standalone.py \
  --data-root ./data \
  --epochs 5 \
  --batch-size 2 \
  --save-dir ./checkpoints
```

## ğŸ“– Usage

### Basic Example

```python
import torch
from sam3_lora import LoRAConfig, inject_lora_into_model
from sam3_lora.model import SimpleSegmentationModel

# 1. Create your model
model = SimpleSegmentationModel()

# 2. Configure LoRA
lora_config = LoRAConfig(
    rank=8,
    alpha=16.0,
    target_modules=["q_proj", "k_proj", "v_proj", "out_proj"]
)

# 3. Inject LoRA
model = inject_lora_into_model(model, lora_config, verbose=True)

# 4. Train as usual!
optimizer = torch.optim.AdamW(
    [p for p in model.parameters() if p.requires_grad],
    lr=1e-4
)
```

### With Training Loop

```python
from sam3_lora.train import SimpleLoRATrainer

# Create trainer
trainer = SimpleLoRATrainer(
    model=model,
    lora_config=lora_config,
    train_loader=train_loader,
    val_loader=val_loader,
    max_epochs=10,
    save_dir="./checkpoints"
)

# Train!
trainer.train()
```

## ğŸ“ Package Structure

```
sam3_lora/
â”œâ”€â”€ sam3_lora/              # Main package
â”‚   â”œâ”€â”€ __init__.py         # Package exports
â”‚   â”œâ”€â”€ lora/               # LoRA implementation
â”‚   â”‚   â”œâ”€â”€ lora_layer.py   # Core LoRA layers
â”‚   â”‚   â””â”€â”€ lora_utils.py   # Utilities
â”‚   â”œâ”€â”€ data/               # Data loading
â”‚   â”‚   â””â”€â”€ dataset.py      # COCO dataset
â”‚   â”œâ”€â”€ model/              # Simple models
â”‚   â”‚   â””â”€â”€ simple_models.py
â”‚   â”œâ”€â”€ train/              # Training
â”‚   â”‚   â””â”€â”€ trainer.py      # Standalone trainer
â”‚   â””â”€â”€ utils/              # Utilities
â”‚       â””â”€â”€ training_utils.py
â”‚
â”œâ”€â”€ data/                   # Your training data
â”‚   â”œâ”€â”€ train/
â”‚   â””â”€â”€ valid/
â”‚
â”œâ”€â”€ setup.py                # Installation script
â”œâ”€â”€ requirements.txt        # Dependencies
â”œâ”€â”€ train_standalone.py     # Standalone training script
â”œâ”€â”€ test_standalone.py      # Test script
â””â”€â”€ README_STANDALONE.md    # This file
```

## ğŸ”§ CLI Commands

### Training

```bash
# Basic training
python3 train_standalone.py --data-root ./data --epochs 10

# With custom settings
python3 train_standalone.py \
  --data-root ./data \
  --rank 16 \
  --alpha 32.0 \
  --epochs 20 \
  --batch-size 4 \
  --lr 1e-4 \
  --save-dir ./my_checkpoints

# Resume training
python3 train_standalone.py \
  --data-root ./data \
  --resume ./checkpoints/best.pt
```

### Testing

```bash
# Test LoRA injection
python3 test_standalone.py

# Expected: All tests pass âœ“
```

## ğŸ“Š What's Included

### LoRA Implementation
- âœ… `LoRALayer` - Core LoRA layer
- âœ… `LinearWithLoRA` - Wrapper for Linear layers
- âœ… `inject_lora_into_model()` - Automatic injection
- âœ… `get_lora_state_dict()` - Save LoRA weights
- âœ… `load_lora_state_dict()` - Load LoRA weights

### Models
- âœ… `SimpleTransformer` - For testing
- âœ… `SimpleSegmentationModel` - For demos

### Training
- âœ… `SimpleLoRATrainer` - Standalone trainer
- âœ… Checkpoint saving/loading
- âœ… Validation support

### Data
- âœ… `LoRASAM3Dataset` - COCO format loader
- âœ… `create_dataloaders()` - Helper function

## ğŸ“ Examples

### Example 1: Inject LoRA into Your Model

```python
from sam3_lora import LoRAConfig, inject_lora_into_model

# Your existing PyTorch model
model = YourModel()

# Add LoRA
lora_config = LoRAConfig(rank=8, alpha=16.0)
model = inject_lora_into_model(model, lora_config)

# Now only LoRA parameters are trainable!
```

### Example 2: Save/Load LoRA Weights

```python
from sam3_lora.lora import get_lora_state_dict, load_lora_state_dict
import torch

# Save only LoRA weights (small file!)
lora_weights = get_lora_state_dict(model)
torch.save(lora_weights, "lora_weights.pt")

# Load into new model
new_model = YourModel()
new_model = inject_lora_into_model(new_model, lora_config)
load_lora_state_dict(new_model, torch.load("lora_weights.pt"))
```

### Example 3: Custom Training

```python
from sam3_lora.train import SimpleLoRATrainer

class MyTrainer(SimpleLoRATrainer):
    def compute_loss(self, batch):
        # Your custom loss
        output = self.model(batch['images'])
        return your_loss_function(output, batch['targets'])

trainer = MyTrainer(model, lora_config, train_loader)
trainer.train()
```

## ğŸ” Differences from Original Version

| Feature | Original | Standalone |
|---------|----------|-----------|
| SAM3 Required | âœ… Yes | âŒ No |
| Installation | Complex | Simple |
| Dependencies | Many | Minimal |
| Use Case | SAM3 only | Any model |
| Size | Large | Small |

## ğŸ“¦ Dependencies

Minimal dependencies:
```
torch>=2.0.0
torchvision>=0.15.0
Pillow>=9.5.0
numpy>=1.24.0
tqdm>=4.65.0
pyyaml>=6.0
tensorboard>=2.12.0
```

No SAM3, Hydra, or other heavy dependencies!

## âš™ï¸ Configuration

### LoRA Parameters

```python
LoRAConfig(
    rank=8,              # Rank (4, 8, 16, 32)
    alpha=16.0,          # Scaling (typically 2*rank)
    dropout=0.1,         # Dropout probability
    target_modules=[     # Which modules to adapt
        "q_proj",
        "k_proj",
        "v_proj",
        "out_proj",
        "linear1",
        "linear2"
    ]
)
```

## ğŸ› Troubleshooting

### Import Error
```python
# âœ— Wrong (old version)
from src.lora import LoRAConfig

# âœ“ Correct (standalone)
from sam3_lora import LoRAConfig
```

### Module Not Found
```bash
# Install the package
cd /workspace/sam3_lora
pip install -e .
```

## ğŸ“š Documentation

- **This File**: Standalone usage guide
- **LORA_IMPLEMENTATION_GUIDE.md**: Detailed technical guide
- **HOW_TO_TRAIN.md**: Training guide
- **CLI_TRAINING_GUIDE.md**: CLI reference

## âœ… Verification

Test that everything works:

```bash
# 1. Install
pip install -e .

# 2. Test
python3 -c "from sam3_lora import LoRAConfig; print('âœ“ Import works')"

# 3. Run full test
python3 test_standalone.py
```

## ğŸ¯ Next Steps

1. **Test**: `python3 test_standalone.py`
2. **Train**: `python3 train_standalone.py --data-root ./data`
3. **Deploy**: Load LoRA weights and use!

## ğŸ“„ License

Same license as SAM3.

## ğŸ™ Credits

Based on:
- **LoRA**: [Hu et al., 2021](https://arxiv.org/abs/2106.09685)
- **SAM3**: Meta AI's Segment Anything Model 3

---

**Status**: âœ… Standalone - No SAM3 Required!
**Version**: 0.1.0
**Python**: 3.8+

ğŸ‰ **This package works independently without SAM3!** ğŸ‰
