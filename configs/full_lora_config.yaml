# SAM3 LoRA Training Configuration
# Full LoRA configuration - applies LoRA to ALL transformer components
# Use this for maximum adaptation capacity (more trainable parameters)

# Model settings
model:
  name: "facebook/sam3"
  cache_dir: null

# LoRA settings
lora:
  rank: 32                   # Higher rank for more capacity
  alpha: 64                  # 2x rank
  dropout: 0.1               # Some dropout for regularization

  # Target more modules including MLP layers
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "out_proj"
    - "fc1"                  # MLP first layer
    - "fc2"                  # MLP second layer

  # Apply LoRA to ALL components
  apply_to_vision_encoder: true
  apply_to_text_encoder: true
  apply_to_geometry_encoder: true
  apply_to_detr_encoder: true
  apply_to_detr_decoder: true
  apply_to_mask_decoder: true

# Training settings
training:
  data_dir: "/workspace/data2"  # Root directory containing train/valid/test folders with COCO annotations
  batch_size: 8         # Smaller batch due to more parameters
  num_workers: 1

  learning_rate: 1e-5        # Much lower LR for initial stability
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1e-8
  max_grad_norm: 1.0

  num_epochs: 200
  warmup_steps: 1000
  lr_scheduler: "cosine"

  logging_steps: 10
  eval_steps: 500
  save_steps: 1000
  save_total_limit: 3

  mixed_precision: "bf16"    # Use bfloat16 if available
  seed: 42
  gradient_accumulation_steps: 4  # Accumulate to simulate larger batch

output:
  output_dir: "outputs/sam3_lora_full"
  logging_dir: "logs"
  save_lora_only: true
  push_to_hub: false
  hub_model_id: null

evaluation:
  metric: "iou"
  save_predictions: false
  compute_metrics_during_training: true

hardware:
  device: "cuda"
  dataloader_pin_memory: true
  use_compile: false
